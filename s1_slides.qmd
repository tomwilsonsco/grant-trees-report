---
title: "Sentinel 1 Time Series Analysis - Woodland Creation Grants"
author: "Tom Wilson"
format: 
  revealjs:
    theme: [default, styles.scss]
    logo: images/SG-logo.jpg
    slide-number: true
    self-contained: false
execute:
  echo: false
---

```{python}
#| echo: false
import pickle
import geopandas as gpd
import pandas as pd
import rasterio as rio
from rasterio.windows import from_bounds
from rasterio.plot import show
from shapely import box
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats import zscore, ttest_ind, linregress
from tabulate import tabulate
from IPython.display import Markdown
from datetime import datetime
from matplotlib_scalebar.scalebar import ScaleBar
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
```

```{python}
#| echo: false
# file paths setup
inputs_dir = Path.cwd().parent / "inputs" 

output_dir = Path.cwd().parent / "outputs"

plots_dir = Path("s1_plots")
```

## Overview {.smaller}

- We analysed Sentinel-1 backscatter time series for conifer and broadleaved grant sites.

- A single Sentinel 1 image offers little for detecting recent tree planting, but a 7-year time series shows interesting patterns.

- We assessed whether two specific time series patterns are consistent across grant sites, enabling development of a monitoring process for Scottish Forestry.

- By analysing a 7-year time series, we can consider if shorter time frames can meet business needs for this type of analysis in future.

## Sentinel 1 - Synthetic Aperture Radar (SAR) {.smaller}

::: columns
::: column
![ESA Sentinel 1 Satellite ([image credit](https://www.thalesgroup.com/en/worldwide/space/news/sentinel-1c-arrives-cannes))](images/Sentinel-1C.jpg){height=525px}
:::

::: column
![Sentinel 1 image visualisation with forestry grant polygon](images/s1.png){height=525px}
:::
:::

## Forestry Grant Scheme Data {.smaller}
```{python}
#| echo: false
conifer_2018 = gpd.read_file(inputs_dir / "FGS_Woodland_Creation_Claims_2018_conifer.gpkg")
blv_2018 = gpd.read_file(inputs_dir / "FGS_Woodland_Creation_Claims_2018_broadleaves.gpkg")
```

- Downloaded from [Scottish Forestry's Open Data Portal](https://open-data-scottishforestry.hub.arcgis.com/datasets/0bde96c1a6a24d5db1b7fb0117e57ec6_0/about).

- Claim year 2018 coincides with first full year of JNCC's Analysis Ready Sentinel 1 imagery.

- Images were extracted and analysed for `{python} len(conifer_2018)` conifer grant schema polygons and `{python} len(blv_2018)` broadleaved.

![](images/SFopen.png){fig-link="https://open-data-scottishforestry.hub.arcgis.com/datasets/0bde96c1a6a24d5db1b7fb0117e57ec6_0/about" fig-align="center" fig-height="300px"}

## Sentinel 1 ARD {.smaller}
- Defra / JNCC Analysis Ready Sentinel 1 GRD imagery available on the [CEDA Archive](https://catalogue.ceda.ac.uk/uuid/05cea0662aa54aa2b7e2c5811e09431f/). 

- Details of the post-processing steps in [user guidance](https://data.jncc.gov.uk/data/dcb14a5e-301f-40ae-94c3-22b73fb4ec57/simple-ard-service-user-guide.pdf).

- First full year of Sentinel 1 ARD is 2018. 

- Images 1 Jan 2018 - 31 Dec 2024 used in this analysis. 

![](images/CEDA.png){fig-align="center"}

## How to Analyse Sentinel 1 GRD images {.smaller}
- Use images captured in ascending and descending orbit separately.

- Use both VV and VH polarisations.

- Pixel values represent radar backscatter intensity in decibels (dB).

- Surface roughness, vegetation structure, and moisture content influence backscatter.

- Tree canopy produces higher backscatter in VV and VH than open land.

---
```{python #orbits-plot}
#| echo: false
plot_extent = gpd.read_file(plots_dir / "example_extent.gpkg")

plot_grants = conifer_2018[conifer_2018.within(plot_extent.iloc[0].geometry)]

saved_arrs = plots_dir / "image_arrays_orbits_plot.pkl"
if not saved_arrs.exists():

    example_imgs = [i for i in image_list if i.split("/")[-4]=="2024"]

    example_imgs = [i for i in example_imgs if i.split("/")[-3]=="06"]

    intersecting_imgs = []
    for i in example_imgs:
        with rio.open(i) as f:
            bounds = box(*f.bounds)
            if plot_extent.iloc[0].geometry.intersects(bounds):
                feature_bounds = plot_extent.geometry.bounds.values[0]
                window = from_bounds(*feature_bounds, f.transform)
                img_arr = f.read(window=window)
                if not np.all(np.isclose(img_arr, 0.0, atol=1e-6)):
                    intersecting_imgs.append(i)
                if 'asc' in intersecting_imgs and 'desc' in intersecting_imgs:
                    break

    asc_img = [i for i in intersecting_imgs if 'asc' in i][0]

    desc_img = [i for i in intersecting_imgs if 'desc' in i][0]

    asc_date = asc_img.split("/")[-1].split("_")[1]

    asc_date = datetime.strptime(asc_date, "%Y%m%d").strftime("%Y-%m-%d")

    desc_date = desc_img.split("/")[-1].split("_")[1]

    desc_date = datetime.strptime(desc_date, "%Y%m%d").strftime("%Y-%m-%d")

    with rio.open(asc_img) as f:
        feature_bounds = plot_extent.geometry.bounds.values[0]
        window = from_bounds(*feature_bounds, f.transform)
        asc_arr = f.read(window=window)
        asc_trans = f.window_transform(window)

    with rio.open(desc_img) as f:
        feature_bounds = plot_extent.geometry.bounds.values[0]
        window = from_bounds(*feature_bounds, f.transform)
        desc_arr = f.read(window=window)
        desc_trans = f.window_transform(window)

    with open(saved_arrs, "wb") as f:
        pickle.dump((asc_arr, asc_trans, asc_date, desc_arr, desc_trans, desc_date), f)
else:
    with open(saved_arrs, "rb") as f:
        asc_arr, asc_trans, asc_date, desc_arr, desc_trans, desc_date = pickle.load(f)
```

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-orbit-plot
#| fig-cap: "Ascending and Descending orbit example (conifer grant 17FGS20737)"
def plot_band(ax, arr, date, orbit, band, transform, vmin, vmax, plot_grants):
    """Helper function to reduce code duplication when plotting each band."""
    show(
        arr,
        ax=ax,
        cmap="viridis",
        title=f"{band} {orbit} {date}",
        transform=transform,
        vmin=vmin,
        vmax=vmax,
    )
    ax.axis("off")
    sb = ScaleBar(dx=1, units="m", location="lower right")
    ax.add_artist(sb)
    plot_grants.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=0.8)

# Create the figure and axes
fig, (asc_plt, desc_plt) = plt.subplots(2, 2, figsize=(8, 8))

# Plot bands for ascending
plot_band(asc_plt[0], asc_arr[0], asc_date, "Ascending", "VV", asc_trans, -15, 0, plot_grants)
plot_band(asc_plt[1], asc_arr[1], asc_date, "Ascending", "VH", asc_trans, -20, -5, plot_grants)

# Plot bands for descending
plot_band(desc_plt[0], desc_arr[0], desc_date, "Descending", "VV", desc_trans, -15, 0, plot_grants)
plot_band(desc_plt[1], desc_arr[1], desc_date, "Descending", "VH", desc_trans, -20, -5, plot_grants)

plt.tight_layout()
plt.show()
```
---
```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-s2-plot
#| fig-cap: "Sentinel 2 true colour optical image (2024-06-05) for comparison (conifer grant 17FGS20737)"


def normalise_window(window_data):
    """
    Normalises window array using class attribute normalise_min and normalise max values.

    Args:
            window_data (numpy.array) Array of image pixel values to normalise.
    :return:
    """
    minv = 0
    maxv = 150
    window_data = np.where(window_data < minv, minv, window_data)
    window_data = np.where(window_data > maxv, maxv, window_data)
    return (window_data - minv) / (maxv - minv)

with rio.open(plots_dir / "s2_example.tif") as f:
    img_arr = f.read()
    img_arr = img_arr[[2, 1, 0], :, :]
    img_arr = normalise_window(img_arr)
    fig, ax = plt.subplots(1, 1, figsize=(4, 4))
    show(img_arr, transform=f.transform, ax=ax)
    ax.axis("off")
    plot_grants.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=0.8)
    sb = ScaleBar(dx=1, units="m", location="lower right")
    ax.add_artist(sb)
    plt.tight_layout()
    plt.show()
```

## Sentinel 1 time series {.smaller}

```{python}
#| echo: false
with open(inputs_dir / "s1_links_all_2018-01-01_2024-12-31.pkl", "rb") as f:
     image_list = pickle.load(f)
img_years = {2018:0, 2019:0,2020:0,2021:0,2022:0, 2023:0, 2024:0}
for img in image_list:
    year = int(img.split("/")[-4])
    img_years[year] += 1
year_tbl = pd.DataFrame(list(img_years.items()), columns=["Year", "S1 images used"])
```

- In total **`{python} len(image_list)`** images were used from 2018 - 2024 from the Sentinel 1 JNCC ARD archive covering the forestry grant schemes selected. 

```{python}
#| echo: false
year_tbl.style.hide(axis="index").set_properties(**{'font-size': '30px'})
```
- From Dec 2021 Sentinel 1B satellite no longer functiong due to [power supply issue](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-1/Mission_ends_for_Copernicus_Sentinel-1B_satellite). Sentinel 1C not launched until late 2024.

## Conifer Time Series {.smaller}
- The FGS claim polygons were used - explode multiparts and retain parts >= 0.5 ha
-  `{python} len(conifer_2018)` conifer grant polygons of claim year 2018. 

For each grant polygon, for each available Sentinel 1 image 2018 - 2024:

1. Extract image pixel values intersecting the grant polygon to an array.
2. Record the image date and orbit (ascending or descending).
3. Take a median of pixel value array.

**Output:** A 2018 - 2024 time series of median pixel values for each grant polygon.
---
```{python}
#| echo: false
results_df = pd.read_pickle(output_dir / f"conifer_claim2018_2018_2024_medians.pkl")
results_df["id"] = results_df["id"].astype(int)

id=422
feature_result = results_df[results_df["id"]==id]

claim_year = conifer_2018[conifer_2018["id"] == int(id)]["claim_year"].values[0]
case_ref = conifer_2018[conifer_2018["id"] == int(id)]["case_ref"].values[0]

color_map = {
    "asc": "#2b9c93",
    "desc": "#e5682a"
}

# Create subplots with a shared x-axis
fig = make_subplots(rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.1)


# Create the VV trace and add its traces to the first subplot
fig_vv = px.line(feature_result, x="date", y="vv", color="orbit", color_discrete_map=color_map,
                 labels={"vv": "VV dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vv.data:
    fig.add_trace(trace, row=1, col=1)

# Create the VH trace, disable its legend entries, and add them to the second subplot
fig_vh = px.line(feature_result, x="date", y="vh", color="orbit", color_discrete_map=color_map,
                 labels={"vh": "VH dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vh.data:
    trace.showlegend = False  # Hide legend for VH traces
    fig.add_trace(trace, row=2, col=1)

# Update layout with an overall title and adjust legend position (if desired)
fig.update_layout(
    title_text="Example conifer grant polygon time series for VV and VH",
    title_x=0.5,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(title="Orbit", orientation="h", x=0.5, y=1.0, xanchor="center"),
    height=700,
    width=1000
)

# Optionally adjust axes styling
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=1, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text= "VV dB", linewidth=2, row=1, col=1)
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=2, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text="VH dB", linewidth=2, row=2, col=1)

fig.show()
```

## Monthly median - conifer {.smaller}
The noisy time series can be smoothed by taken a monthly median of the feature values. 

---
```{python}
#| echo: false
# Load data
results_df = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

# Filter for a specific ID
id = 422
feature_result = results_df[results_df["id"] == id]

# Ensure date is a datetime object
feature_result.loc[:,"date"] = pd.to_datetime(feature_result["date"])

color_map = {
    "asc": "#2b9c93",
    "desc": "#e5682a"
}

# Create subplots with a shared x-axis
fig = make_subplots(rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.1)


# Create the VV trace and add its traces to the first subplot
fig_vv = px.line(feature_result, x="date", y="vv", color="orbit", color_discrete_map=color_map,
                 labels={"vv": "VV dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vv.data:
    fig.add_trace(trace, row=1, col=1)

# Create the VH trace, disable its legend entries, and add them to the second subplot
fig_vh = px.line(feature_result, x="date", y="vh", color="orbit", color_discrete_map=color_map,
                 labels={"vh": "VH dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vh.data:
    trace.showlegend = False  # Hide legend for VH traces
    fig.add_trace(trace, row=2, col=1)

# Update layout with an overall title and adjust legend position (if desired)
fig.update_layout(
    title_text="Example conifer grant polygon monthly median time series for VV and VH",
    title_x=0.5,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(title="Orbit", orientation="h", x=0.5, y=1.0, xanchor="center"),
    height=700,
    width=1000
)

# Optionally adjust axes styling
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=1, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text= "VV dB", linewidth=2, row=1, col=1)
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=2, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text="VH dB", linewidth=2, row=2, col=1)

fig.show()
```
## Time series trends - conifer {.smaller}
Two trends were apparent in the time series for many 2018 claim sites:

1.  A spike in VV, not VH seen before Spring 2020. Possibly ground prep for planting?

2.  An overall upward trend in VV and VH backscatter, particularly in years 2020 - 2024. This is generally stronger in VH.

How to quantify the proportion of the `{python} len(conifer_2018)` (2018 claim) conifer grants showing these time series patterns?

## Quantifying the time series trends - conifer {.smaller}

The aim is to quantify the two time series trends for all grants. Specifically, to determine:

-   How many of the `{python} len(conifer_2018)` conifer grant polygons with claim year 2018 show a spike in VV by March 2020, which may indicate ground preparation for tree planting.

-   A general increase in backscatter in both polarisations, but particularly in VH indicating trees are growing and a tree canopy is developing.

## Identifying spike in VV early in time series (conifer ground prep) {.smaller}

- Using Z-scores to analyse the 2018 - 2024 time series of monthly medians.

$$
z = \frac{x - \mu}{\sigma}
$$

where:

-   $x$ is the observed back scatter for a given month,
-   $\mu$ is the mean of the monthly backscatter values over the entire time series,
-   $\sigma$ is the standard deviation of these monthly backscatter values.

---
```{python}
# Load data
results_df = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

# Filter for a specific ID
id = 422
feature_result = results_df[results_df["id"] == id].copy()

# Ensure date is a datetime object
feature_result.loc[:,"date"] = pd.to_datetime(feature_result["date"])

feature_result["zscore_vv"] = feature_result.groupby("orbit")["vv"].transform(zscore)
feature_result["zscore_vh"] = feature_result.groupby("orbit")["vh"].transform(zscore)

color_map = {
    "asc": "#2b9c93",
    "desc": "#e5682a"
}

# Create subplots with a shared x-axis
fig = make_subplots(rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.1)

# Create the VV trace and add its traces to the first subplot
fig_vv = px.line(feature_result, x="date", y="zscore_vv", color="orbit", color_discrete_map=color_map, 
                 labels={"vv": "VV dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vv.data:
    trace.showlegend = False
    fig.add_trace(trace, row=1, col=1)

# Create the VH trace, disable its legend entries, and add them to the second subplot
fig_vh = px.line(feature_result, x="date", y="zscore_vh", color="orbit", color_discrete_map=color_map,
                 labels={"vh": "VH dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vh.data:
    fig.add_trace(trace, row=2, col=1)

# Update layout with an overall title and adjust legend position (if desired)
fig.update_layout(
    title_text="Example conifer grant polygon - Z scores of monthly median time series for VV and VH",
    title_x=0.5,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(title="Orbit", orientation="h", x=0.5, y=1.0, xanchor="center"),
    height=700,
    width=1000
)

# Optionally adjust axes styling
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=1, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text= "Z Score VV", linewidth=2, row=1, col=1)
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=2, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text="Z Score VH", linewidth=2, row=2, col=1)

fig.show()
```

## Z score threshold and assumptions - conifer {.smaller}
- A Z Score threshold of 2 was chosen. 
- A VV time series spike, indicated by Z score of >= 2, should occur by spring 2020 (for our test).
- If a 2018 claim conifer site does not have this, indicates no ground prep for planting?

## Z score threshold results - conifer {.smaller}
- The z-scores were calcuated per grant polygon for VV, VH for ascending, descending orbits for the 2018 - 2024 time series of monthly medians. 

- The percentage of the `{python} len(conifer_2018)` conifer 2018 claim polygons having a z-score of 2.0 or greater (in either orbit) at three annual intervals to 31 March 2020 are shown below. 

```{python}
#| echo: false
monthly_medians = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

monthly_medians["year_month"] = pd.to_datetime(monthly_medians["year_month"])

# Calculate Z-scores for VV and VH within each (id, orbit) group
monthly_medians["vv_zscore"] = monthly_medians.groupby(["id", "orbit"])["vv"].transform(zscore)
monthly_medians["vh_zscore"] = monthly_medians.groupby(["id", "orbit"])["vh"].transform(zscore)

# Detect spikes based on Z-score > 2
monthly_medians["vv_spike"] = monthly_medians["vv_zscore"] >= 2.0
monthly_medians["vh_spike"] = monthly_medians["vh_zscore"] >= 2.0

# Define cumulative date ranges
date_ranges = ["2018-03-31", "2019-03-31", "2020-03-31"]
summary_data = []

for date in date_ranges:
    df_spikes = monthly_medians[monthly_medians["year_month"] <= date]
    oids_with_vv_spike = df_spikes[df_spikes["vv_spike"]]["id"].unique()
    oids_with_vh_spike = df_spikes[df_spikes["vh_spike"]]["id"].unique()
    
    summary_data.append([
        date,
        len(oids_with_vv_spike) / monthly_medians["id"].nunique() * 100,
        len(oids_with_vh_spike) / monthly_medians["id"].nunique() * 100
    ])

# Create a summary DataFrame
summary_df = pd.DataFrame(summary_data, columns=["Date to", "VV Spike (%)", "VH Spike (%)"])
```


```{python}
#| echo: false
#| tbl-colwidths: [1,1]

def format_it(val):
    if not isinstance(val, float):
        return val
    else:
        return f"{val:.2f}"

summary_df.style.format(format_it).hide(axis="index").set_properties(**{'font-size': '30px'})

```

## Summary - Ground prep spike in VV - conifers {.smaller}
 - 84% of 1124 conifer grants show the spike in VV within 2¼ years.
 - Is this detecting ground preparation activities carried out before conifer planting?
 - Consistent with VV polarisation being more sensitive to regular textures associated with man-made features, rather than vegetation.
 - Need to repeat - compare non-planting, other years, understand timings of typical process on the ground. 

## Quantifying increase in backscatter over time series {.smaller}

 - As the canopy develops, it introduces [more volume scattering from branches and leaves](https://www.nature.com/articles/s41597-021-01059-7) - increased backscatter in the cross-polarised (VH) channel.
 - Look at overall trend in backscatter time series separately from seasonal or short term variation.
 - Generally higher backscatter in VV and VH in winter than summer, but not consistent.
 - A 12-month rolling median was used to smooth the time series.
---
```{python}
#| echo: false
df = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

df["year_month"] = pd.to_datetime(df["year_month"])

df = df.sort_values(by=["id", "orbit", "year_month"])

df["vv_trend"] = df.groupby(["id", "orbit"])["vv"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

df["vh_trend"] = df.groupby(["id", "orbit"])["vh"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

# Filter for a specific ID
id = 422
feature_result = df[df["id"] == id].copy()

color_map = {
    "asc": "#2b9c93",
    "desc": "#e5682a"
}

# Create subplots with a shared x-axis
fig = make_subplots(rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.1)

# Create the VV trace and add its traces to the first subplot
fig_vv = px.line(feature_result, x="date", y="vv_trend", color="orbit", color_discrete_map=color_map, 
                 labels={"vv_trend": "VV dB (smoothed)", "date": "Date", "orbit": "Orbit"})
for trace in fig_vv.data:
    trace.showlegend = False
    fig.add_trace(trace, row=1, col=1)

# Create the VH trace, disable its legend entries, and add them to the second subplot
fig_vh = px.line(feature_result, x="date", y="vh_trend", color="orbit", color_discrete_map=color_map,
                 labels={"vh_trend": "VH dB (smoothed)", "date": "Date", "orbit": "Orbit"})
for trace in fig_vh.data:
    fig.add_trace(trace, row=2, col=1)

# Update layout with an overall title and adjust legend position (if desired)
fig.update_layout(
    title_text="Example conifer grant polygon - smoothed monthly median time series for VV and VH",
    title_x=0.5,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(title="Orbit", orientation="h", x=0.5, y=1.0, xanchor="center"),
    height=700,
    width=1000
)

# Optionally adjust axes styling
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=1, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text= "Z Score VV", linewidth=2, row=1, col=1)
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=2, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text="Z Score VH", linewidth=2, row=2, col=1)

fig.show()
```

## Applying linear regression - conifer {.smaller}
- Linear regression was applied to assess the strength and statistical significance of (increasing) trend in backscatter.

- Excluded first couple of years, so for 2018 claim, used 1 January 2020 to 31 December 2024.

- Linear regressions were fitted to the smoothed VV and VH backscatter time series for each site (conifer grant polygon), and for each orbit (ascending and descending).

## Linear regression results - conifer {.smaller}
- Percentage conifer 2018 claim sites showing a statistically significant (p < 0.05) positive trend in VV and VH backscatter since January 2020, along with the mean slope (dB/month) for those sites.

```{python}
#| echo: false
# Filter to data from 1 Jan 2020 onwards
df = df[df["year_month"] >= "2020-01-01"]

# Sort before regression
df = df.sort_values(["id", "orbit", "year_month"])

# Container for results
results = []

# Group by (id, orbit)
for (oid, orbit), group in df.groupby(["id", "orbit"]):
    group = group.copy()
    group["time_index"] = (group["year_month"] - pd.Timestamp("2020-01-01")).dt.days / 30.44

    # Drop rows where both VV and VH are missing
    valid_rows = group.dropna(subset=["vv_trend", "vh_trend"])
    if len(valid_rows) < 6:
        continue  # Skip short time series

    # Regression for VV
    vv_mask = valid_rows["vv_trend"].notna()
    x_vv = valid_rows.loc[vv_mask, "time_index"]
    y_vv = valid_rows.loc[vv_mask, "vv_trend"]

    if len(x_vv) >= 3:
        slope_vv, _, _, p_value_vv, _ = linregress(x_vv, y_vv)
    else:
        slope_vv, p_value_vv = float("nan"), float("nan")

    # Regression for VH
    vh_mask = valid_rows["vh_trend"].notna()
    x_vh = valid_rows.loc[vh_mask, "time_index"]
    y_vh = valid_rows.loc[vh_mask, "vh_trend"]

    if len(x_vh) >= 3:
        slope_vh, _, _, p_value_vh, _ = linregress(x_vh, y_vh)
    else:
        slope_vh, p_value_vh = float("nan"), float("nan")

    # Store results
    results.append({
        "id": oid,
        "orbit": orbit,
        "vv_slope": slope_vv,
        "vv_p_value": p_value_vv,
        "vh_slope": slope_vh,
        "vh_p_value": p_value_vh
    })

# Compile result
regression_results = pd.DataFrame(results)
```

```{python}
#| echo: false
#| tbl-colwidths: [1,1,1]

# Get total number of unique IDs
total_ids = regression_results["id"].nunique()

# VV: Filter for significant positive slope
vv_sig = regression_results[
    (regression_results["vv_slope"] > 0) & (regression_results["vv_p_value"] < 0.05)
]
vv_percent = vv_sig["id"].nunique() / total_ids * 100
vv_mean_slope = vv_sig["vv_slope"].mean()

# VH: Same for VH
vh_sig = regression_results[
    (regression_results["vh_slope"] > 0) & (regression_results["vh_p_value"] < 0.05)
]
vh_percent = vh_sig["id"].nunique() / total_ids * 100
vh_mean_slope = vh_sig["vh_slope"].mean()

# Create summary table
summary_df = pd.DataFrame({
    "Polarisation": ["VV", "VH"],
    "% Sites +ve Trend (p < 0.05)": [round(vv_percent, 2), round(vh_percent, 2)],
    "Mean Slope (+ve, p < 0.05)": [round(vv_mean_slope, 4), round(vh_mean_slope, 4)]
})

styled_table = (
    summary_df
    .style
    .format({"% Sites +ve Trend (p < 0.05)": lambda x: f"{x:.2f}" if isinstance(x, float) else x,
             "Mean Slope (dB / month, p < 0.05 and +ve)": lambda x: f"{x:.4f}" if isinstance(x, float) else x})
    .hide(axis="index")
    .set_table_styles([{'selector': 'th', 'props': [('font-size', '25px')]}])
    .set_properties(**{'font-size': '25px'})
)
styled_table
```

- 90% of 2018 conifer grants show significant positive trend in VH backscatter since January 2020 (51% for VV). VH has greater average slope than VV.

- These results support the use of VH over VV for long-term monitoring of tree growth. Aligns with what is seen in other studies.

## Broadleaves time series analysis {.smaller}
The same process applied to conifers was repeated for broadleaves / native broadleaves grants:

- `{python} len(blv_2018)` broadleaved grant polygons with claim year 2018 were analysed (after exploding multiparts and removing parts < 0.5 ha).  
- 7 years of Sentinel 1 images 2018 - 2024 were extracted for these polygons as masked arrays.  
- Time series were contstructed separately for ascending and descending orbits and then convered into monthly medians.  
---
```{python}
#| echo: false

# Load data
results_df = pd.read_csv(output_dir / "broadleaves_monthly_medians_grant_year2018_all.csv")

# Filter for a specific ID
id = 300
feature_result = results_df[results_df["id"] == id]

# Ensure date is a datetime object
feature_result.loc[:,"date"] = pd.to_datetime(feature_result["date"])

color_map = {
    "asc": "#2b9c93",
    "desc": "#e5682a"
}

# Create subplots with a shared x-axis
fig = make_subplots(rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.1)

# Create the VV trace and add its traces to the first subplot
fig_vv = px.line(feature_result, x="date", y="vv", color="orbit", color_discrete_map=color_map, 
                 labels={"vv": "VV dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vv.data:
    trace.showlegend = False
    fig.add_trace(trace, row=1, col=1)

# Create the VH trace, disable its legend entries, and add them to the second subplot
fig_vh = px.line(feature_result, x="date", y="vh", color="orbit", color_discrete_map=color_map,
                 labels={"vh": "VH dB", "date": "Date", "orbit": "Orbit"})
for trace in fig_vh.data:
    fig.add_trace(trace, row=2, col=1)

# Update layout with an overall title and adjust legend position (if desired)
fig.update_layout(
    title_text="Example broadleaved grant polygon - monthly median time series for VV and VH",
    title_x=0.5,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(title="Orbit", orientation="h", x=0.5, y=1.0, xanchor="center"),
    height=700,
    width=1000
)

# Optionally adjust axes styling
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=1, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text= "Z Score VV", linewidth=2, row=1, col=1)
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=2, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text="Z Score VH", linewidth=2, row=2, col=1)

fig.show()
```

## Observed trends - broadleaves {.smaller}
- The spike in VV polarisation - seen for conifers January 2018 to March 2020 - was not seen as frequently for 2018 claim broadleaved grants.  
- A gradual increase in backscatter observed 2020 - 2024, particularly in VH polarisation. This was not as strong a trend as for conifers, probably because broadleaves take longer to become established and grow.
- Repeated conifer analysis for broadleaved 2018 grants.
- In future more customised methods could be investigated for broadleaves.

## Z-score analysis - broadleaves {.smaller}
The conifer grant z-score analysis was repeated - looking for VV and VH time series spikes before 31 March 2020.

```{python}
#| echo: false
monthly_medians = pd.read_csv(output_dir / "broadleaves_monthly_medians_grant_year2018_all.csv")

monthly_medians["year_month"] = pd.to_datetime(monthly_medians["year_month"])

# Calculate Z-scores for VV and VH within each (id, orbit) group
monthly_medians["vv_zscore"] = monthly_medians.groupby(["id", "orbit"])["vv"].transform(zscore)
monthly_medians["vh_zscore"] = monthly_medians.groupby(["id", "orbit"])["vh"].transform(zscore)

# Detect spikes based on Z-score > 2
monthly_medians["vv_spike"] = monthly_medians["vv_zscore"] >= 2.0
monthly_medians["vh_spike"] = monthly_medians["vh_zscore"] >= 2.0

# Define cumulative date ranges
date_ranges = ["2018-03-31", "2019-03-31", "2020-03-31"]
summary_data = []

for date in date_ranges:
    df_spikes = monthly_medians[monthly_medians["year_month"] <= date]
    oids_with_vv_spike = df_spikes[df_spikes["vv_spike"]]["id"].unique()
    oids_with_vh_spike = df_spikes[df_spikes["vh_spike"]]["id"].unique()
    
    summary_data.append([
        date,
        len(oids_with_vv_spike) / monthly_medians["id"].nunique() * 100,
        len(oids_with_vh_spike) / monthly_medians["id"].nunique() * 100
    ])

# Create a summary DataFrame
summary_df = pd.DataFrame(summary_data, columns=["Date to", "VV Spike (%)", "VH Spike (%)"])
```


```{python}
#| echo: false

def format_it(val):
    if not isinstance(val, float):
        return val
    else:
        return f"{val:.2f}"

styled_table = summary_df.style.format(format_it).hide(axis="index")
styled_table
```
- VV polarisation the Z-score >= 2 spike was observed for 20% fewer sites for broadleaves (64% compared to 84% for conifers). Assume the same ground preparation not typical when planting broadleaves. 

- If so, a 20% difference in the occurence between conifer and broadleaved sites is perhaps not as great as expected.

## Quantifying increase over the time series - broadleaves {.smaller}
- Quantify the number of broadleaved grant sites showing an increase in VV and VH using linear regression. 

- The process described for conifers was applied to the broadleaved sites.
- Firstly a monthly smoothing.

---
```{python}
#| echo: false
df = pd.read_csv(output_dir / "broadleaves_monthly_medians_grant_year2018_all.csv")

df["year_month"] = pd.to_datetime(df["year_month"])

df = df.sort_values(by=["id", "orbit", "year_month"])

df["vv_trend"] = df.groupby(["id", "orbit"])["vv"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

df["vh_trend"] = df.groupby(["id", "orbit"])["vh"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

# Filter for a specific ID
id = 300
feature_result = df[df["id"] == id].copy()

color_map = {
    "asc": "#2b9c93",
    "desc": "#e5682a"
}

# Create subplots with a shared x-axis
fig = make_subplots(rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.1)

# Create the VV trace and add its traces to the first subplot
fig_vv = px.line(feature_result, x="date", y="vv_trend", color="orbit",          color_discrete_map=color_map, 
                 labels={"vv_trend": "VV dB (smoothed)", "date": "Date", "orbit": "Orbit"})
for trace in fig_vv.data:
    trace.showlegend = False
    fig.add_trace(trace, row=1, col=1)

# Create the VH trace, disable its legend entries, and add them to the second subplot
fig_vh = px.line(feature_result, x="date", y="vh_trend", color="orbit", color_discrete_map=color_map,
                 labels={"vh_trend": "VH dB (smoothed)", "date": "Date", "orbit": "Orbit"})
for trace in fig_vh.data:
    fig.add_trace(trace, row=2, col=1)

# Update layout with an overall title and adjust legend position (if desired)
fig.update_layout(
    title_text="Example broadleaved grant polygon - smoothed monthly median time series for VV and VH",
    title_x=0.5,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(title="Orbit", orientation="h", x=0.5, y=1.0, xanchor="center"),
    height=700,
    width=1000
)

# Optionally adjust axes styling
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=1, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text= "Z Score VV", linewidth=2, row=1, col=1)
fig.update_xaxes(showline=True, linecolor="black", title_text="", linewidth=2, row=2, col=1)
fig.update_yaxes(showline=True, linecolor="black", title_text="Z Score VH", linewidth=2, row=2, col=1)

fig.show()
```

## Linear regression - broadleaves {.smaller}

- As for conifers, to quantify trend for all broadleaved sites, a linear regression was applied to the 1 January 2020 to 31 Dec 2024 portion of smoothed time series.

- The table shows the proportion of the `{python} len(blv_2018)` broadleaved sites having a significant (p < 0.05) positive trend.

```{python}
#| echo: false
# Filter to data from 1 Jan 2020 onwards
df = df[df["year_month"] >= "2020-01-01"]

# Sort before regression
df = df.sort_values(["id", "orbit", "year_month"])

# Container for results
results = []

# Group by (id, orbit)
for (oid, orbit), group in df.groupby(["id", "orbit"]):
    group = group.copy()
    group["time_index"] = (group["year_month"] - pd.Timestamp("2020-01-01")).dt.days / 30.44

    # Drop rows where both VV and VH are missing
    valid_rows = group.dropna(subset=["vv_trend", "vh_trend"])
    if len(valid_rows) < 6:
        continue  # Skip short time series

    # Regression for VV
    vv_mask = valid_rows["vv_trend"].notna()
    x_vv = valid_rows.loc[vv_mask, "time_index"]
    y_vv = valid_rows.loc[vv_mask, "vv_trend"]

    if len(x_vv) >= 3:
        slope_vv, _, _, p_value_vv, _ = linregress(x_vv, y_vv)
    else:
        slope_vv, p_value_vv = float("nan"), float("nan")

    # Regression for VH
    vh_mask = valid_rows["vh_trend"].notna()
    x_vh = valid_rows.loc[vh_mask, "time_index"]
    y_vh = valid_rows.loc[vh_mask, "vh_trend"]

    if len(x_vh) >= 3:
        slope_vh, _, _, p_value_vh, _ = linregress(x_vh, y_vh)
    else:
        slope_vh, p_value_vh = float("nan"), float("nan")

    # Store results
    results.append({
        "id": oid,
        "orbit": orbit,
        "vv_slope": slope_vv,
        "vv_p_value": p_value_vv,
        "vh_slope": slope_vh,
        "vh_p_value": p_value_vh
    })

# Compile result
regression_results = pd.DataFrame(results)
```

```{python}
#| echo: false

# Get total number of unique IDs
total_ids = regression_results["id"].nunique()

# VV: Filter for significant positive slope
vv_sig = regression_results[
    (regression_results["vv_slope"] > 0) & (regression_results["vv_p_value"] < 0.05)
]
vv_percent = vv_sig["id"].nunique() / total_ids * 100
vv_mean_slope = vv_sig["vv_slope"].mean()

# VH: Same for VH
vh_sig = regression_results[
    (regression_results["vh_slope"] > 0) & (regression_results["vh_p_value"] < 0.05)
]
vh_percent = vh_sig["id"].nunique() / total_ids * 100
vh_mean_slope = vh_sig["vh_slope"].mean()

# Create summary table
summary_df = pd.DataFrame({
    "Polarisation": ["VV", "VH"],
    "% Sites +ve Trend (p < 0.05)": [round(vv_percent, 2), round(vh_percent, 2)],
    "Mean Slope (dB / month, p < 0.05 and +ve)": [round(vv_mean_slope, 4), round(vh_mean_slope, 4)]
})

styled_table = (
    summary_df
    .style
    .format({"% Sites +ve Trend (p < 0.05)": lambda x: f"{x:.2f}" if isinstance(x, float) else x,
             "Mean Slope (dB / month, p < 0.05 and +ve)": lambda x: f"{x:.4f}" if isinstance(x, float) else x})
    .hide(axis="index")
    .set_table_styles([{'selector': 'th', 'props': [('font-size', '25px')]}])
    .set_properties(**{'font-size': '25px'})
)

styled_table
```
- Positive trend slightly less strong for broadleaves, probably because broadleaves grow more slowly. 

## Conclusions - Sentinel 1 Time Series Analysis {.smaller}
A time series analysis of Sentinel 1 GRD imagery applied to forestry grant schemes can be used to:

- Show when and if ground prepartion has occurred by looking for a spike in the time series for VV polarisation within 1 - 2 years of the grant claim year. 

- Show if trees are growing on the site by applying linear regression to a smoothed time series over years 2 - 7 and looking for significant positive trends, particularly in VH polarisation. A minimum 4-5 years from claim year is needed.

## Conclusions - Sentinel 1 Time Series Analysis {.smaller}
In future:  

- **Business use case**: Further understanding of forestry practices, details of business needs would help to refine analytical methods. 

- **Ground truth data**: Future analysis comparing a time series for non-planted sites with otherwise similar characteristics would be invaluable.

- More advanced time series analysis and machine learning methods may be used once business requirements and ground truth patterns are understood.