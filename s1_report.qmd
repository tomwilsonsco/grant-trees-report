---
title: "Sentinel 1 Forestry Grant Scheme Analysis"
author: "Tom Wilson"
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 3
    number-sections: true
    page-layout: article
---

```{python}
#| echo: false
import pickle
import geopandas as gpd
import pandas as pd
import rasterio as rio
from rasterio.windows import from_bounds
from rasterio.plot import show
from shapely import box
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats import zscore, ttest_ind, linregress
from tabulate import tabulate
from IPython.display import Markdown
from datetime import datetime
from matplotlib_scalebar.scalebar import ScaleBar
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
```

```{python}
#| echo: false
# file paths setup
inputs_dir = Path.cwd().parent / "inputs" 

output_dir = Path.cwd().parent / "outputs"

plots_dir = Path("s1_plots")
```

## Introduction

A time series of Sentinel 1 Synthetic Aperture Radar (SAR) Ground Range Detected (GRD) images were explored against Scottish Forestry grant scheme polygons. The aim was to explore how a time series of Sentinel 1 images over several years might be analysed to detect that grant-funded trees have been planted and are growing in the first few years after planting. Immediately after planting, trees are too small to be discerned on Sentinel 2 optical imagery (10m) and 3-4 metre Planet Scope imagery. 

Radar imagery is well suited to time series analysis because it isn’t affected by cloud cover. In Scotland, Sentinel 1 images are available least weekly. With the Sentinel-1 satellites now operating for nearly 10 years, a substantial time series can be extracted for analysis. While Sentinel-1 radar backscatter may not directly detect newly planted trees, ground disturbance from operations like ploughing, ripping, and mounding before planting may be detected early on and provide useful information that tree planting operations have started.

Monitoring these patterns in Sentinel 1 is dependent on administrative grant polygon data. The grant boundaries provide small targeted windows for image pixel extraction making data processing easier. They also provide fixed spatial units for analysing radar imagery backscatter over time and applying statistical techniques.

## Forestry Grant Scheme polygons

Forestry Grant Scheme Claims are available for download from the [Scottish Forestry Open Data Portal](https://open-data-scottishforestry.hub.arcgis.com/datasets/0bde96c1a6a24d5db1b7fb0117e57ec6_0/about).

For this initial study, the set of grant schemes analysed were:

-   Claim year 2018 (claim year is the financial year work will be completed and grant claimed).

-   Options: Conifer (the most frequent type) and Broadleaves / Native Broadleaves. The Native Broadleaves grant option is more frequent than just Broadleaves, so the two were combined as one 'Broadleaves' category here. 

The aim was to extract a time series of Sentinel 1 imagery pixel values (backscatter in VV and VH polarisations) for these polygons. To make the imagery extraction process easier, multipart grant polygons were exploded to single part and only polygons of \>= 0.5 ha were analysed.

```{python}
#| echo: false
conifer_2018 = gpd.read_file(inputs_dir / "FGS_Woodland_Creation_Claims_2018_conifer.gpkg")
blv_2018 = gpd.read_file(inputs_dir / "FGS_Woodland_Creation_Claims_2018_broadleaves.gpkg")
```

Claim year 2018 was chosen as this coincided with the earliest full year of JNCC's Analysis Ready Sentinel 1 imagery.

Images were extracted and analysed for `{python} len(conifer_2018)` conifer grant schema polygons and `{python} len(blv_2018)` broadleaved.

## Sentinel 1 Analysis Ready GRD imagery

Defra / JNCC have prepared Analysis Ready Sentinel 1 GRD imagery and this is available on the [CEDA Archive](https://catalogue.ceda.ac.uk/uuid/05cea0662aa54aa2b7e2c5811e09431f/). Details of the post-processing steps taken by JNCC to make the analysis ready product are available in the [user guidance](https://data.jncc.gov.uk/data/dcb14a5e-301f-40ae-94c3-22b73fb4ec57/simple-ard-service-user-guide.pdf).

On examining this collection, it was found that the first complete year of imagery was 2018 and it is still being added to in 2025. For this analysis, images available from 1 January 2018 to 31 December 2024 were analysed.

As Sentinel 1 Radar imagery are not affected by cloud cover, all available images can be extracted and used.

```{python}
#| echo: false
with open(inputs_dir / "s1_links_all_2018-01-01_2024-12-31.pkl", "rb") as f:
     image_list = pickle.load(f)
img_years = {2018:0, 2019:0,2020:0,2021:0,2022:0, 2023:0, 2024:0}
for img in image_list:
    year = int(img.split("/")[-4])
    img_years[year] += 1
year_tbl = pd.DataFrame(list(img_years.items()), columns=["Year", "S1 images used"])
```

In total **`{python} len(image_list)`** images were used from 2018 - 2024 from the Sentinel 1 JNCC ARD archive covering the forestry grant schemes selected. As shown in @tbl-images, the number of images per year approximately halved after 2021 when the Sentinel 1B satellite [suffered a power supply issue](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-1/Mission_ends_for_Copernicus_Sentinel-1B_satellite).

```{python}
#| echo: false
#| label: tbl-images
#| tbl-cap: "The number of Sentinel 1 images used per year for this analysis" 
#| tbl-colwidths: [1,1]
#Markdown(tabulate(list(img_years.items()), tablefmt="pipe", headers=["Year", "S1 images #used for forestry grant analysis"], showindex=False))
year_tbl.style.hide(axis="index")
```

A dedicated analysis environment is not available for processing the JNCC / Defra ARD imagery. [Custom code](https://github.com/tomwilsonsco/ceda_sentinel) was created to extract image arrays for the grant polygons and ran on a local laptop. It took about 24 hours to extract the 7 years of Sentinel 1 imagery for `{python} len(conifer_2018)` conifer grant polygons. 

## Sentinel 1 Imagery Details

It is important to understand some features of the Sentinel 1 imagery used in this analysis:

-   SAR imagery is a side looking senor. Images are captured on the ascending and descending orbit passes of the satellite. It is important to consider ascending and descending images separately as they can show different backscatter values, particularly where the ground is not flat. Ascending and descending images are typically captured on separate days. An example is shown in @fig-orbit-plot and can be contrasted with @fig-s2-plot, a true colour Sentinel 2 image of the same location.

-   The JNCC Sentinel 1 ARD imagery is provided in 10m resolution and in VV and VH polarisations. The VV polarisation is the vertical transmit and vertical receive polarisation and VH is vertical transmit and horizontal receive. These means for any location on the ground we may consider four values from Sentinel 1: VV ascending, VH ascending, VV descending, VH descending.

-   In the Sentinel 1 GRD collection processed by JNCC, pixel values represent radar backscatter intensity in decibels (dB). The values are in a logarithmic scale, indicating the strength of the returned radar signal. Higher backscatter values suggest stronger surface reflections, which can be influenced by factors such as surface roughness, vegetation structure, and moisture content.

-   In generally, backscatter is higher in each polarisation from mature standing trees than from bare ground or open fields. The VV polarisation is more sensitive to regular, man-made structures like buildings. The VH polarisation is more sensitive to volume scattering from vegetation. Generally, the VH polarisation is [more sensitive to irregular surfaces like forests](https://sentiwiki.copernicus.eu/web/s1-applications#S1Applications-ForestryS1-Applications-Forestry).

```{python #orbits-plot}
#| echo: false
plot_extent = gpd.read_file(plots_dir / "example_extent.gpkg")

plot_grants = conifer_2018[conifer_2018.within(plot_extent.iloc[0].geometry)]

saved_arrs = plots_dir / "image_arrays_orbits_plot.pkl"
if not saved_arrs.exists():

    example_imgs = [i for i in image_list if i.split("/")[-4]=="2024"]

    example_imgs = [i for i in example_imgs if i.split("/")[-3]=="06"]

    intersecting_imgs = []
    for i in example_imgs:
        with rio.open(i) as f:
            bounds = box(*f.bounds)
            if plot_extent.iloc[0].geometry.intersects(bounds):
                feature_bounds = plot_extent.geometry.bounds.values[0]
                window = from_bounds(*feature_bounds, f.transform)
                img_arr = f.read(window=window)
                if not np.all(np.isclose(img_arr, 0.0, atol=1e-6)):
                    intersecting_imgs.append(i)
                if 'asc' in intersecting_imgs and 'desc' in intersecting_imgs:
                    break

    asc_img = [i for i in intersecting_imgs if 'asc' in i][0]

    desc_img = [i for i in intersecting_imgs if 'desc' in i][0]

    asc_date = asc_img.split("/")[-1].split("_")[1]

    asc_date = datetime.strptime(asc_date, "%Y%m%d").strftime("%Y-%m-%d")

    desc_date = desc_img.split("/")[-1].split("_")[1]

    desc_date = datetime.strptime(desc_date, "%Y%m%d").strftime("%Y-%m-%d")

    with rio.open(asc_img) as f:
        feature_bounds = plot_extent.geometry.bounds.values[0]
        window = from_bounds(*feature_bounds, f.transform)
        asc_arr = f.read(window=window)
        asc_trans = f.window_transform(window)

    with rio.open(desc_img) as f:
        feature_bounds = plot_extent.geometry.bounds.values[0]
        window = from_bounds(*feature_bounds, f.transform)
        desc_arr = f.read(window=window)
        desc_trans = f.window_transform(window)

    with open(saved_arrs, "wb") as f:
        pickle.dump((asc_arr, asc_trans, asc_date, desc_arr, desc_trans, desc_date), f)
else:
    with open(saved_arrs, "rb") as f:
        asc_arr, asc_trans, asc_date, desc_arr, desc_trans, desc_date = pickle.load(f)
```

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-orbit-plot
#| fig-cap: "Ascending and Descending orbit example (conifer grant 17FGS20737)"
def plot_band(ax, arr, date, orbit, band, transform, vmin, vmax, plot_grants):
    """Helper function to reduce code duplication when plotting each band."""
    show(
        arr,
        ax=ax,
        cmap="viridis",
        title=f"{band} {orbit} {date}",
        transform=transform,
        vmin=vmin,
        vmax=vmax,
    )
    ax.axis("off")
    sb = ScaleBar(dx=1, units="m", location="lower right")
    ax.add_artist(sb)
    plot_grants.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=0.8)

# Create the figure and axes
fig, (asc_plt, desc_plt) = plt.subplots(2, 2, figsize=(8, 8))

# Plot bands for ascending
plot_band(asc_plt[0], asc_arr[0], asc_date, "Ascending", "VV", asc_trans, -15, 0, plot_grants)
plot_band(asc_plt[1], asc_arr[1], asc_date, "Ascending", "VH", asc_trans, -20, -5, plot_grants)

# Plot bands for descending
plot_band(desc_plt[0], desc_arr[0], desc_date, "Descending", "VV", desc_trans, -15, 0, plot_grants)
plot_band(desc_plt[1], desc_arr[1], desc_date, "Descending", "VH", desc_trans, -20, -5, plot_grants)

plt.tight_layout()
plt.savefig("images/plot.png", dpi=100)
plt.show()
```
```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-s2-plot
#| fig-cap: "Sentinel 2 true colour optical image (2024-06-05) for comparison (conifer grant 17FGS20737)"


def normalise_window(window_data):
    """
    Normalises window array using class attribute normalise_min and normalise max values.

    Args:
            window_data (numpy.array) Array of image pixel values to normalise.
    :return:
    """
    minv = 0
    maxv = 150
    window_data = np.where(window_data < minv, minv, window_data)
    window_data = np.where(window_data > maxv, maxv, window_data)
    return (window_data - minv) / (maxv - minv)

with rio.open(plots_dir / "s2_example.tif") as f:
    img_arr = f.read()
    img_arr = img_arr[[2, 1, 0], :, :]
    img_arr = normalise_window(img_arr)
    fig, ax = plt.subplots(1, 1, figsize=(4, 4))
    show(img_arr, transform=f.transform, ax=ax)
    ax.axis("off")
    plot_grants.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=0.8)
    sb = ScaleBar(dx=1, units="m", location="lower right")
    ax.add_artist(sb)
    plt.tight_layout()
    plt.gcf().set_dpi(100)
    plt.show()
```

## Conifer grants time series analysis

### Creating the time series

For the `{python} len(conifer_2018)` conifer grant polygons of claim year 2018 (after exploding multiparts and removing parts \< 0.5 ha), 7 years of Sentinel 1 images 2018 - 2024 were extracted as masked arrays. The image date and ascending or descending orbit information were retained, so it was possible to construct a time series and analyse the two orbits separately.

A median of the pixel values per grant polygon feature was taken. This resulted in a pair of VV and VH values per image date, for each image intersecting each of the 1128 polygons over the 7 years.

The full time series of all images for a feature has a lot of noise as shown in the example @fig-all-ts. Sentinel 1 and SAR images have a feature [known as speckle](https://forum.sentinel-hub.com/t/sentinel-1-speckle-filter-support/4206).

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-all-ts
#| fig-cap: "Example time series for one conifer grant feature without temporal smoothing"
#| fig-subcap:
#|   - "VV polarisation"
#|   - "VH polarisation"
#| fig-cap-location: bottom
#| include_plotlyjs: false


results_df = pd.read_pickle(output_dir / f"conifer_claim2018_2018_2024_medians.pkl")
results_df["id"] = results_df["id"].astype(int)

id=422
feature_result = results_df[results_df["id"]==id]

claim_year = conifer_2018[conifer_2018["id"] == int(id)]["claim_year"].values[0]
case_ref = conifer_2018[conifer_2018["id"] == int(id)]["case_ref"].values[0]

# Figure a VV
fig_vv = px.line(
    feature_result, 
    x="date", 
    y="vv", 
    color="orbit",
    labels={"vv": "VV dB", "date": "Date", "orbit": "Orbit"}
)

fig_vv.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vv.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vv.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VV figure
fig_vv.show()

# Figure b VH
fig_vh = px.line(
    feature_result, 
    x="date", 
    y="vh", 
    color="orbit",  # Differentiate ascending vs. descending
    labels={"vh": "VH dB", "date": "Date", "orbit": "Orbit"}
)

fig_vh.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vh.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vh.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VH figure
fig_vh.show()
```

The noisy time series can be smoothed by taken a monthly median of the feature values. The result for the same example feature is shown in @fig-monthly-ts.

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-monthly-ts
#| fig-cap: "Example time series for one conifer grant feature using a monthly median"
#| fig-subcap:
#|    - "VV monthly median"
#|    - "VH monthly median"
#| fig-cap-location: bottom

# Load data
results_df = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

# Filter for a specific ID
id = 422
feature_result = results_df[results_df["id"] == id]

# Ensure date is a datetime object
feature_result.loc[:,"date"] = pd.to_datetime(feature_result["date"])


# Figure a: VV

fig_vv = px.line(
    feature_result, 
    x="date", 
    y="vv", 
    color="orbit",
    labels={"vv": "VV dB", "date": "Date", "orbit": "Orbit"}
)

fig_vv.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vv.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vv.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VV figure
fig_vv.show()


# Figure b: VH
fig_vh = px.line(
    feature_result, 
    x="date", 
    y="vh", 
    color="orbit",
    labels={"vh": "VH dB", "date": "Date", "orbit": "Orbit"}
)

fig_vh.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vh.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vh.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VH figure
fig_vh.show()
```

### Observed trends

By reviewing the monthly median time series for a number of different conifer grant polygons, two general trends are apparent in some, but not all sites:

1.  The time series often shows a spike in winter / spring 2018 or 2019 and always seen before spring 2020. This spike is mainly visible in the VV polarisation, not VH. This is thought to be the ground preparation for planting (mounding, ploughing, ripping). It makes sense this is visible mostly in VV which is known to be more sensitive to regular, man-made features, whereas VH is more sensitive to changes in vegetation.

2.  The time series over 7 years shows an upward trend in backscatter, particularly in years 2020 - 2024. This is generally stronger in VH polarisation and this is assumed to be as the trees become established they are responsible for greater backscatter, particularly seen in VH.

### Quantifying the time series trends

The aim is to quantify the two time series trends for all grants. Specifically, to determine:

-   How many of the 1128 conifer grant polygons with claim year 2018 show a spike in VV by March 2020, which may indicate ground preparation for tree planting.

-   A general increase in backscatter in both polarisations, but particularly in VH indicating trees are growing and a tree canopy is developing.

#### Quantifying potential ground preparation spike in VV backscatter

In this early exploration, a very simple statistical method was applied to the time series. By converting the monthly time series into z-scores, it is possible to analyse individual monthly spikes in VV in the first two to three years from 2018. Using a z-score means a spike in backscatter can be found for a site relative to its seven year time series, rather than applying a fixed decibel threshold across all sites.

The z-score formula is:

$$
z = \frac{x - \mu}{\sigma}
$$

where:

-   $x$ is the observed median back scatter for a given month,
-   $\mu$ is the mean of the monthly backscatter values over the entire time series,
-   $\sigma$ is the standard deviation of these monthly backscatter values.

A z-score threshold of 2.0 was used. A value above this (i.e. $z$ \> 2) indicates that the monthly backscatter is more than two standard deviations above the mean, suggesting a time series spike. A z-score of 2 is not an extreme outlier in a normal distribution. However, as backscatter values naturally increase in the later years of the seven-year time series (as tree growth progresses), it was felt a threshold of $z$ \= 2 was suitable for this analysis applied to the first two years.

If the January 2018 to March 2024 monthly median values do not have a spike in VV that is z-score of 2.0 or greater then this might show no ground preparation for conifer planting has occurred in the location.

@fig-monthly-zscore shows a time series of monthly Z-score values for the same site as @fig-monthly-ts.

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-monthly-zscore
#| fig-cap: "Example time series for one conifer grant feature using a zscore of monthly median values"
#| fig-subcap:
#|    - "Z Score VV monthly median"
#|    - "Z Score VH monthly median"
#| fig-cap-location: bottom

# Load data
results_df = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

# Filter for a specific ID
id = 422
feature_result = results_df[results_df["id"] == id].copy()

# Ensure date is a datetime object
feature_result.loc[:,"date"] = pd.to_datetime(feature_result["date"])

feature_result["zscore_vv"] = feature_result.groupby("orbit")["vv"].transform(zscore)
feature_result["zscore_vh"] = feature_result.groupby("orbit")["vh"].transform(zscore)

# Figure a: VV

fig_vv = px.line(
    feature_result, 
    x="date", 
    y="zscore_vv", 
    color="orbit",
    labels={"zscore_vv": "VV z-score", "date": "Date", "orbit": "Orbit"}
)

fig_vv.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vv.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vv.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VV figure
fig_vv.show()


# Figure b: VH
fig_vh = px.line(
    feature_result, 
    x="date", 
    y="zscore_vh", 
    color="orbit",  # Differentiate ascending vs. descending
    labels={"zscore_vh": "VH z-score", "date": "Date", "orbit": "Orbit"}
)

fig_vh.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vh.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vh.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VH figure
fig_vh.show()
```

Z-scores were calcuated per grant polygon for VV and for VH for ascending and descending orbits. The percentage of the `{python} len(conifer_2018)` conifer 2018 claim polygons having a z-score of 2.0 or greater (in either orbit) at three annual intervals to 31 March 2020 are shown in @tbl-zscore-spike. 

```{python}
#| echo: false
monthly_medians = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

monthly_medians["year_month"] = pd.to_datetime(monthly_medians["year_month"])

# Calculate Z-scores for VV and VH within each (id, orbit) group
monthly_medians["vv_zscore"] = monthly_medians.groupby(["id", "orbit"])["vv"].transform(zscore)
monthly_medians["vh_zscore"] = monthly_medians.groupby(["id", "orbit"])["vh"].transform(zscore)

# Detect spikes based on Z-score > 2
monthly_medians["vv_spike"] = monthly_medians["vv_zscore"] >= 2.0
monthly_medians["vh_spike"] = monthly_medians["vh_zscore"] >= 2.0

# Define cumulative date ranges
date_ranges = ["2018-03-31", "2019-03-31", "2020-03-31"]
summary_data = []

for date in date_ranges:
    df_spikes = monthly_medians[monthly_medians["year_month"] <= date]
    oids_with_vv_spike = df_spikes[df_spikes["vv_spike"]]["id"].unique()
    oids_with_vh_spike = df_spikes[df_spikes["vh_spike"]]["id"].unique()
    
    summary_data.append([
        date,
        len(oids_with_vv_spike) / monthly_medians["id"].nunique() * 100,
        len(oids_with_vh_spike) / monthly_medians["id"].nunique() * 100
    ])

# Create a summary DataFrame
summary_df = pd.DataFrame(summary_data, columns=["Date to", "VV Spike (%)", "VH Spike (%)"])
```


```{python}
#| echo: false
#| label: tbl-zscore-spike
#| tbl-cap: "Percentage of 2018 claim Conifer grants showing a monthly spike in VV and VH by different dates." 
#| tbl-colwidths: [1,1]

def format_it(val):
    if not isinstance(val, float):
        return val
    else:
        return f"{val:.2f}"

styled_table = summary_df.style.format(format_it).hide(axis="index")
styled_table
```

The results in @tbl-zscore-spike show a clear difference between VV and VH polarisations, with VV much more likely to exhibit a spike in backscatter during the first couple of years after a conifer grant claim. Over 80% of `{python} len(conifer_2018)` sites showing the spike in VV means it seems likely they are due to a change on the ground. It may be because the sensor is detecting ground preparation activities carried out before conifer planting. This interpretation would be consistent with the fact that VV polarisation is generally more sensitive to regular textures associated with man-made features.

However, before drawing firm conclusions, it would be important to repeat the analysis for other grant claim years, as well as for nearby sites where no tree planting took place. Speaking with staff at Scottish Forestry to understand a typical operational cycle of conifer planting would also be helpful.

#### Quantifying general increase in backscatter over the time series
Another trend observed in the time series was a general increase in backscatter over the seven-year period, particularly in the VH polarisation. This is likely associated with the growth and establishment of conifers. As the canopy develops, it introduces [more volume scattering from branches and leaves](https://www.nature.com/articles/s41597-021-01059-7), which contributes to increased diffuse backscatter in the cross-polarised (VH) channel.

To prepare the seven-year time series for trend analysis, it is first important to reduce the influence of seasonal variation. In general, the conifer planting sites show higher backscatter in both VV and VH during winter compared to summer. However, this pattern is not consistent across all sites, as illustrated in the example in @fig-monthly-ts, where a VH peak is seen in June 2020. Given this variability, a seasonal decomposition method was not applied. Instead, a 12-month rolling median was used to smooth the time series. The effect of this rolling median on the example site is shown in @fig-monthly-rolling.

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-monthly-rolling
#| fig-cap: "Example time series for one conifer grant feature applying rolling 12 month smoothing to the monthly median values"
#| fig-subcap:
#|    - "VV monthly median - rolling 12 month smoothing"
#|    - "VH monthly median - rolling 12 month smoothing"
#| fig-cap-location: bottom


df = pd.read_csv(output_dir / "conifer_monthly_medians_grant_year2018_all.csv")

df["year_month"] = pd.to_datetime(df["year_month"])

df = df.sort_values(by=["id", "orbit", "year_month"])

df["vv_trend"] = df.groupby(["id", "orbit"])["vv"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

df["vh_trend"] = df.groupby(["id", "orbit"])["vh"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

# Filter for a specific ID
id = 422
feature_result = df[df["id"] == id].copy()

# Figure a: VV

fig_vv = px.line(
    feature_result, 
    x="date", 
    y="vv_trend", 
    color="orbit",
    labels={"vv_trend": "VV dB (smoothed)", "date": "Date", "orbit": "Orbit"}
)

fig_vv.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vv.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vv.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VV figure
fig_vv.show()


# Figure b: VH
fig_vh = px.line(
    feature_result, 
    x="date", 
    y="vh_trend", 
    color="orbit",  # Differentiate ascending vs. descending
    labels={"vh_trend": "VH dB (smoothed)", "date": "Date", "orbit": "Orbit"}
)

fig_vh.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vh.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vh.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VH figure
fig_vh.show()
```

After smoothing the monthly median time series using a 12-month rolling median, linear regression was applied to assess the strength and statistical significance of trends in backscatter. For the 2018 conifer grant sites, the analysis was restricted to 1 January 2020 to 31 December 2024, rather than using the full time series starting in 2018. This excludes the initial spike—particularly evident in VV backscatter described previously. No trend was expected in the first couple of years immediately following planting, so the 2020 starting point for 2018 claim year grants seemed reasonable.

Separate linear regressions were fitted to the VV and VH backscatter time series for each site (conifer grant polygon), and for each orbit (ascending and descending) independently.

The regression results were then summarised to report the percentage of sites that showed a positive trend with a p-value below 0.05 in either orbit. These results for VV and VH are presented in @tbl-trends.

```{python}
#| echo: false
# Filter to data from 1 Jan 2020 onwards
df = df[df["year_month"] >= "2020-01-01"]

# Sort before regression
df = df.sort_values(["id", "orbit", "year_month"])

# Container for results
results = []

# Group by (id, orbit)
for (oid, orbit), group in df.groupby(["id", "orbit"]):
    group = group.copy()
    group["time_index"] = (group["year_month"] - pd.Timestamp("2020-01-01")).dt.days / 30.44

    # Drop rows where both VV and VH are missing
    valid_rows = group.dropna(subset=["vv_trend", "vh_trend"])
    if len(valid_rows) < 6:
        continue  # Skip short time series

    # Regression for VV
    vv_mask = valid_rows["vv_trend"].notna()
    x_vv = valid_rows.loc[vv_mask, "time_index"]
    y_vv = valid_rows.loc[vv_mask, "vv_trend"]

    if len(x_vv) >= 3:
        slope_vv, _, _, p_value_vv, _ = linregress(x_vv, y_vv)
    else:
        slope_vv, p_value_vv = float("nan"), float("nan")

    # Regression for VH
    vh_mask = valid_rows["vh_trend"].notna()
    x_vh = valid_rows.loc[vh_mask, "time_index"]
    y_vh = valid_rows.loc[vh_mask, "vh_trend"]

    if len(x_vh) >= 3:
        slope_vh, _, _, p_value_vh, _ = linregress(x_vh, y_vh)
    else:
        slope_vh, p_value_vh = float("nan"), float("nan")

    # Store results
    results.append({
        "id": oid,
        "orbit": orbit,
        "vv_slope": slope_vv,
        "vv_p_value": p_value_vv,
        "vh_slope": slope_vh,
        "vh_p_value": p_value_vh
    })

# Compile result
regression_results = pd.DataFrame(results)
```

```{python}
#| echo: false
#| label: tbl-trends
#| tbl-cap: "Percentage conifer 2018 claim sites showing a statistically significant (p < 0.05) positive trend in VV and VH backscatter since January 2020, along with the mean slope (dB/month) for those sites"
#| tbl-colwidths: [1,1,1]

# Get total number of unique IDs
total_ids = regression_results["id"].nunique()

# VV: Filter for significant positive slope
vv_sig = regression_results[
    (regression_results["vv_slope"] > 0) & (regression_results["vv_p_value"] < 0.05)
]
vv_percent = vv_sig["id"].nunique() / total_ids * 100
vv_mean_slope = vv_sig["vv_slope"].mean()

# VH: Same for VH
vh_sig = regression_results[
    (regression_results["vh_slope"] > 0) & (regression_results["vh_p_value"] < 0.05)
]
vh_percent = vh_sig["id"].nunique() / total_ids * 100
vh_mean_slope = vh_sig["vh_slope"].mean()

# Create summary table
summary_df = pd.DataFrame({
    "Polarisation": ["VV", "VH"],
    "% Sites Positive Trend (where p < 0.05)": [round(vv_percent, 2), round(vh_percent, 2)],
    "Mean Slope (where p < 0.05 and +ve)": [round(vv_mean_slope, 4), round(vh_mean_slope, 4)]
})

styled_table = (
    summary_df
    .style
    .format({"% Sites Positive Trend (where p < 0.05)": lambda x: f"{x:.2f}" if isinstance(x, float) else x,
             "Mean Slope (where p < 0.05 and +ve)": lambda x: f"{x:.4f}" if isinstance(x, float) else x})
    .hide(axis="index")
)
styled_table
```

As seen in @tbl-trends, 90% of the 2018 claim conifer grant sites showed a statistically significant positive trend in VH backscatter since January 2020. In contrast, only 51% of sites showed a comparable trend in VV backscatter. The mean trend or slope is also less strong for VV. This shows that for monitoring grant sites in the longer term VH is more useful than VV and agrees with what is described elsewhere that VH is more sensitive to tree and vegetation growth.

## Broadleaves time series analysis
### Creating the time series
The same process applied to conifers was repeated for broadleaves / native broadleaves grants:

- In total `{python} len(blv_2018)` broadleaved grant polygons with claim year 2018 were analysed (after exploding multiparts and removing parts < 0.5 ha).  
- 7 years of Sentinel 1 images 2018 - 2024 were extracted for these polygons as masked arrays.  
- Time series were contstructed separately for ascending and descending orbits and then convered into monthly medians. An example is shown in @fig-blv-monthly-ts.  

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-blv-monthly-ts
#| fig-cap: "Example time series for one broadleaved grant feature using a monthly median"
#| fig-subcap:
#|    - "VV monthly median"
#|    - "VH monthly median"
#| fig-cap-location: bottom

# Load data
results_df = pd.read_csv(output_dir / "broadleaves_monthly_medians_grant_year2018_all.csv")

# Filter for a specific ID
id = 300
feature_result = results_df[results_df["id"] == id]

# Ensure date is a datetime object
feature_result.loc[:,"date"] = pd.to_datetime(feature_result["date"])


# Figure a: VV

fig_vv = px.line(
    feature_result, 
    x="date", 
    y="vv", 
    color="orbit",
    labels={"vv": "VV dB", "date": "Date", "orbit": "Orbit"}
)

fig_vv.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vv.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vv.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VV figure
fig_vv.show()


# Figure b: VH
fig_vh = px.line(
    feature_result, 
    x="date", 
    y="vh", 
    color="orbit",
    labels={"vh": "VH dB", "date": "Date", "orbit": "Orbit"}
)

fig_vh.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vh.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vh.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VH figure
fig_vh.show()
```

### Observed trends - broadleaves
- The spike in VV polarisation - seen for conifers in the first couple of years January 2018 to March 2020 - was not seen as frequently for broadleaves.  
- There was a gradual increase in backscatter observed over the last few years, particularly in VH polarisation. This was not as strong a trend as for conifers, probably because broadleaves take longer to become established and grow.

### Quantifying the time series trends - broadleaves
The same simple statistical analyses were repeated for the broadleaved grants. With more time to explore the patterns, it might be that specific processes could be developed for broadleaves. Working with Scottish Forestry staff to understand the planting process on the ground would help.

Firstly the [conifer z-score process](#quantifying-potential-ground-preparation-spike-in-vv-backscatter) was repeated, looking for VV and VH spikes before 31 March 2020.

```{python}
#| echo: false
monthly_medians = pd.read_csv(output_dir / "broadleaves_monthly_medians_grant_year2018_all.csv")

monthly_medians["year_month"] = pd.to_datetime(monthly_medians["year_month"])

# Calculate Z-scores for VV and VH within each (id, orbit) group
monthly_medians["vv_zscore"] = monthly_medians.groupby(["id", "orbit"])["vv"].transform(zscore)
monthly_medians["vh_zscore"] = monthly_medians.groupby(["id", "orbit"])["vh"].transform(zscore)

# Detect spikes based on Z-score > 2
monthly_medians["vv_spike"] = monthly_medians["vv_zscore"] >= 2.0
monthly_medians["vh_spike"] = monthly_medians["vh_zscore"] >= 2.0

# Define cumulative date ranges
date_ranges = ["2018-03-31", "2019-03-31", "2020-03-31"]
summary_data = []

for date in date_ranges:
    df_spikes = monthly_medians[monthly_medians["year_month"] <= date]
    oids_with_vv_spike = df_spikes[df_spikes["vv_spike"]]["id"].unique()
    oids_with_vh_spike = df_spikes[df_spikes["vh_spike"]]["id"].unique()
    
    summary_data.append([
        date,
        len(oids_with_vv_spike) / monthly_medians["id"].nunique() * 100,
        len(oids_with_vh_spike) / monthly_medians["id"].nunique() * 100
    ])

# Create a summary DataFrame
summary_df = pd.DataFrame(summary_data, columns=["Date to", "VV Spike (%)", "VH Spike (%)"])
```


```{python}
#| echo: false
#| label: tbl-blv-zscore-spike
#| tbl-cap: "Percentage of 2018 claim broadleaved grants showing a monthly spike in VV and VH by different dates." 
#| tbl-colwidths: [1,1]

def format_it(val):
    if not isinstance(val, float):
        return val
    else:
        return f"{val:.2f}"

styled_table = summary_df.style.format(format_it).hide(axis="index")
styled_table
```

For VV polarisation the Z-score >= 2 spike was observed for 20% fewer sites for broadleaves (64% compared to 84% for conifers). It is assumed the same ground prepartion tasks are not frequently carried out for broadleaves. 

A 20% difference in the occurence between conifer and broadleaved sites is perhaps not as great as expected. We would need to learn more about the difference in planting practices from Scottish Forestry. 

#### Quantifying general increase in backscatter over the time series - broadleaves
As described [for conifers](#quantifying-general-increase-in-backscatter-over-the-time-series), the next analysis was to quantify the number of broadleaved grant sites showing an increase in VV and VH using linear regression. The same process was applied to the broadleaved sites, firstly a monthly smoothing as shown in @fig-blv-monthly-rolling for the same example plotted in @fig-blv-monthly-ts.

```{python}
#| echo: false
#| column: page-inset-right
#| label: fig-blv-monthly-rolling
#| fig-cap: "Example time series for one broadleaved grant feature applying rolling 12 month smoothing to the monthly median values"
#| fig-subcap:
#|    - "VV monthly median - rolling 12 month smoothing"
#|    - "VH monthly median - rolling 12 month smoothing"
#| fig-cap-location: bottom


df = pd.read_csv(output_dir / "broadleaves_monthly_medians_grant_year2018_all.csv")

df["year_month"] = pd.to_datetime(df["year_month"])

df = df.sort_values(by=["id", "orbit", "year_month"])

df["vv_trend"] = df.groupby(["id", "orbit"])["vv"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

df["vh_trend"] = df.groupby(["id", "orbit"])["vh"].transform(
    lambda x: x.rolling(window=12, center=True, min_periods=1).median()
)

# Filter for a specific ID
id = 300
feature_result = df[df["id"] == id].copy()

# Figure a: VV

fig_vv = px.line(
    feature_result, 
    x="date", 
    y="vv_trend", 
    color="orbit",
    labels={"vv_trend": "VV dB (smoothed)", "date": "Date", "orbit": "Orbit"}
)

fig_vv.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vv.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vv.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VV figure
fig_vv.show()


# Figure b: VH
fig_vh = px.line(
    feature_result, 
    x="date", 
    y="vh_trend", 
    color="orbit",  # Differentiate ascending vs. descending
    labels={"vh_trend": "VH dB (smoothed)", "date": "Date", "orbit": "Orbit"}
)

fig_vh.update_layout(
    height=400, width=750,
    paper_bgcolor="white",
    plot_bgcolor="white",
    legend=dict(
        orientation="h",
        yanchor="bottom", y=1.02,
        xanchor="center", x=0.5
    )
)
fig_vh.update_xaxes(showline=True, linecolor="black", linewidth=2)
fig_vh.update_yaxes(showline=True, linecolor="black", linewidth=2)

# Show the VH figure
fig_vh.show()
```

It can be seen in @fig-blv-monthly-rolling when compared with a conifer example in @fig-monthly-rolling that the positive trend might be less strong for broadleaves, probably because the trees grow more slowly. 

For each site, a linear regression was applied to the backscatter values from 1 January 2020 to 31 Dec 2024 (after smoothing with the rolling monthly median). This means results in @tbl-blv-trends for broadleaves are directly comparable with those for conifers (in @tbl-trends).  @tbl-blv-trends shows the proportion of the `{python} len(blv_2018)` broadleaved sites having a significant (p < 0.05) positive trend.

```{python}
#| echo: false
# Filter to data from 1 Jan 2020 onwards
df = df[df["year_month"] >= "2020-01-01"]

# Sort before regression
df = df.sort_values(["id", "orbit", "year_month"])

# Container for results
results = []

# Group by (id, orbit)
for (oid, orbit), group in df.groupby(["id", "orbit"]):
    group = group.copy()
    group["time_index"] = (group["year_month"] - pd.Timestamp("2020-01-01")).dt.days / 30.44

    # Drop rows where both VV and VH are missing
    valid_rows = group.dropna(subset=["vv_trend", "vh_trend"])
    if len(valid_rows) < 6:
        continue  # Skip short time series

    # Regression for VV
    vv_mask = valid_rows["vv_trend"].notna()
    x_vv = valid_rows.loc[vv_mask, "time_index"]
    y_vv = valid_rows.loc[vv_mask, "vv_trend"]

    if len(x_vv) >= 3:
        slope_vv, _, _, p_value_vv, _ = linregress(x_vv, y_vv)
    else:
        slope_vv, p_value_vv = float("nan"), float("nan")

    # Regression for VH
    vh_mask = valid_rows["vh_trend"].notna()
    x_vh = valid_rows.loc[vh_mask, "time_index"]
    y_vh = valid_rows.loc[vh_mask, "vh_trend"]

    if len(x_vh) >= 3:
        slope_vh, _, _, p_value_vh, _ = linregress(x_vh, y_vh)
    else:
        slope_vh, p_value_vh = float("nan"), float("nan")

    # Store results
    results.append({
        "id": oid,
        "orbit": orbit,
        "vv_slope": slope_vv,
        "vv_p_value": p_value_vv,
        "vh_slope": slope_vh,
        "vh_p_value": p_value_vh
    })

# Compile result
regression_results = pd.DataFrame(results)
```

```{python}
#| echo: false
#| label: tbl-blv-trends
#| tbl-cap: "Percentage conifer 2018 claim sites showing a statistically significant (p < 0.05) positive trend in VV and VH backscatter since January 2020, along with the mean slope (dB/month) for those sites"
#| tbl-colwidths: [1,1,1]

# Get total number of unique IDs
total_ids = regression_results["id"].nunique()

# VV: Filter for significant positive slope
vv_sig = regression_results[
    (regression_results["vv_slope"] > 0) & (regression_results["vv_p_value"] < 0.05)
]
vv_percent = vv_sig["id"].nunique() / total_ids * 100
vv_mean_slope = vv_sig["vv_slope"].mean()

# VH: Same for VH
vh_sig = regression_results[
    (regression_results["vh_slope"] > 0) & (regression_results["vh_p_value"] < 0.05)
]
vh_percent = vh_sig["id"].nunique() / total_ids * 100
vh_mean_slope = vh_sig["vh_slope"].mean()

# Create summary table
summary_df = pd.DataFrame({
    "Polarisation": ["VV", "VH"],
    "% Sites Positive Trend (where p < 0.05)": [round(vv_percent, 2), round(vh_percent, 2)],
    "Mean Slope (where p < 0.05 and +ve)": [round(vv_mean_slope, 4), round(vh_mean_slope, 4)]
})

styled_table = (
    summary_df
    .style
    .format({"% Sites Positive Trend (where p < 0.05)": lambda x: f"{x:.2f}" if isinstance(x, float) else x,
             "Mean Slope (where p < 0.05 and +ve)": lambda x: f"{x:.4f}" if isinstance(x, float) else x})
    .hide(axis="index")
)

styled_table
```